---
title: "R Notebook"
output: html_notebook
---

```{r}
library(jsonlite)
library(dplyr)
library(utils)
library(stringi)
```

```{r}
# This merges the cfimdb and base files so that they exist in one csv file

csfimdb <- read.delim("ids-cfimdb-train.csv")
base <- read.delim("ids-sst-train.csv")

# Transform labels of cfimdb
csfimdb$sentiment = ifelse(csfimdb$sentiment == 1, 4, 1)

csfimdb <- csfimdb %>%
  filter(sentiment == 4)

df_merged <- rbind(base, csfimdb)
df_merged <- df_merged %>%
  select(id, sentence, sentiment)

# Make sure that csv is tab delimited
write.table(df_merged, "ids-sst-train-merged.csv", row.names = FALSE, sep="\t")
```

```{r}
# Adds duplicate question dataset 


duplicate_questions_raw <- fromJSON("stackexchange_duplicatequestions.json")
colnames(duplicate_questions_raw) <- c("sentence1", "sentence2")

set.seed(123)

duplicate_questions <- as.data.frame(duplicate_questions_raw,
                                     row.names = c("sentence1", "sentence2"))

# Generate random ids
duplicate_questions$id <- stri_rand_strings(nrow(duplicate_questions), 
                                            50,
                                            pattern="[0-9a-f]")

# Set is_duplicate to 1.0
duplicate_questions <- duplicate_questions[0:50000,]

duplicate_questions <- duplicate_questions %>%
  mutate(is_duplicate = 1.0) %>%
  select(id, sentence1, sentence2, is_duplicate)

# Just get the first 1000 observations
write.csv(duplicate_questions, "para-train-duplicate-questions.csv", 
            row.names = FALSE, sep="\t")
```
```{r}

# Make sure that csv is tab delimited

# Add merged para

para_base <- read.delim("quora-train.csv")

para_merged <- rbind(para_base, duplicate_questions)

# Make sure that csv is tab delimited
write.table(para_merged, "para-train-merged.csv", 
            row.names = FALSE, sep="\t")

```

```{r}
# Add cosine similarity loss data
sts_raw <- read.delim("sts-train.csv")

# 1 for equivalent sentences, 0 for non equivalent sentences
cos_sim <- sts_raw %>%
  filter(similarity >= 4.0| similarity < 0.8) %>%
  mutate(equivalence = ifelse(similarity >= 4.0, 1, -1)) %>%
  select(id, sentence1, sentence2, equivalence)

# Make sure that csv is tab delimited
write.csv(cos_sim, "cos-sim-train.csv", 
            row.names = FALSE, sep="\t")

```

```{r}
# Add negative rankings loss data
sts_raw <- read.delim("sts-train.csv")

neg_rankings <- sts_raw %>%
  filter(similarity >= 3.5) %>%
  select(id, sentence1, sentence2, similarity)

# Make sure that csv is tab delimited
write.csv(neg_rankings, "neg-rankings-train.csv", 
            row.names = FALSE, sep="\t")

```





Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

